import streamlit as st

st.title("Л M茅todos de Optimizaci贸n Basados en Gradiente")

st.markdown("""
<style>
.justify {
    text-align: justify;
    font-size: 16px;
    line-height: 1.7em;
    color: #333333;
}
</style>

<div class="justify">
<p>
Los <strong>m茅todos de optimizaci贸n basados en gradiente</strong> son t茅cnicas fundamentales en el an谩lisis num茅rico y en el aprendizaje autom谩tico. Su prop贸sito es encontrar los m铆nimos (o m谩ximos) de funciones continuas mediante el uso de informaci贸n derivada: el <strong>gradiente</strong> o vector de derivadas parciales.
</p>

<p>
Estos m茅todos parten de un punto inicial y se mueven iterativamente en la direcci贸n opuesta al gradiente, ya que este indica el sentido de mayor incremento de la funci贸n. Al avanzar en sentido contrario, se busca minimizar la funci贸n..
</p>

<p>
La ventaja principal de estos m茅todos radica en su <strong>velocidad de convergencia</strong>, especialmente cuando la funci贸n es suave y diferenciable. Adem谩s, su estructura permite aplicarlos en problemas de gran escala, siendo la base de algoritmos modernos como el descenso estoc谩stico y el m茅todo Adam.
</p>

<p>
Entre los enfoques m谩s conocidos se encuentran:
</p>

<p><strong> Descenso del Gradiente (Gradient Descent):</strong> se mueve directamente en la direcci贸n negativa del gradiente, ajustando el tama帽o del paso (learning rate) en cada iteraci贸n.</p>

<p><strong> M茅todo de Cauchy:</strong> tambi茅n conocido como el m茅todo del gradiente m谩s r谩pido, busca un paso 贸ptimo mediante una minimizaci贸n unidimensional en la direcci贸n del gradiente.</p>

<p><strong> M茅todo de Newton:</strong> utiliza la segunda derivada (Hessiano) para ajustar la direcci贸n de avance, permitiendo una convergencia cuadr谩tica cerca del m铆nimo si la matriz es invertible.</p>

<p><strong> M茅todo quasi-Newton:</strong> construye una aproximaci贸n del Hessiano de forma m谩s eficiente, como lo hace el algoritmo BFGS.</p>

<p>
Estos m茅todos ofrecen un equilibrio entre precisi贸n y rapidez, aunque pueden presentar dificultades si el gradiente es costoso de calcular, si existen m煤ltiples 贸ptimos o si la funci贸n tiene valles estrechos o planos.
</p>

<p>
Esta aplicaci贸n te permitir谩 visualizar c贸mo evolucionan distintas estrategias basadas en gradientes frente a varias funciones objetivo, y c贸mo cada una ajusta su trayecto hasta alcanzar un punto 贸ptimo.
</p>
</div>
""", unsafe_allow_html=True)
